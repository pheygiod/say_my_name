{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655eb4fe-00ec-4a88-8d20-78fbdbaffedd",
   "metadata": {},
   "source": [
    "# When Data Breaks Bad â€” Analysing Breaking Bad Dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2d2dd-2163-40e3-bc11-bed733127a68",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f33960-b4c6-42fc-9036-507bb00611d5",
   "metadata": {},
   "source": [
    "In this project, we aim to analyse Breaking Bad transcripts to identify patterns. We'll extract data from an online source and organise it into a dataframe. Our goal is to uncover insights such as:\n",
    "\n",
    "- Most speaking characters per season.\n",
    "- Top 8 characters by dialogue frequency.\n",
    "- Top 3 characters with the shortest and longest sentences.\n",
    "- Season with the highest number of sentences.\n",
    "- Relationship between sentences per character and sentence length.\n",
    "- Episode theme identification through cluster analysis.\n",
    "- Common interactions among characters via network analysis of dialogue exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60441f3b-f026-466b-b79a-312d6c086a6b",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc3e1b-648a-459b-b484-724b7a380d35",
   "metadata": {},
   "source": [
    "Here are the main conclusions we reached so far:\n",
    "\n",
    "- The top 3 characters with the longest sentences are Hank, Walter, and Skyler. The top 3 characters with the shortest sentences are 'Everyone' (i.e., a group of people), Skyler, and Walter.\n",
    "- Out of all the seasons we managed to analyse, season 2 is the longest. Yet, we only managed to analyse season 1 and 2 and part of season 3. We couldn't analyse season 4 and 5 due to the character's names issue.\n",
    "- We found a linear correlation between longest sentences and the number of sentences. So he more sentences a character says, the longer their largest sentence is. This is because the main characters often speak profound and deep sentences.\n",
    "- We discovered that Jesse and Walter Jr have most distinguished clusters. Characters speaking similarly include women in love. For instance, Skyler, Marie, Jane. Also young characters, such as Walter Jr and Jane. Finally, characters who discuss similar topics (i.e., Walter, Hank, and Saul).\n",
    "- The most frequent conversation happens between Walter and Jesse. The second one is between Walter and Skyler. The third strongest relationship is the one between Jesse and Jane.\n",
    "\n",
    "For more details, please refer to the the full analysis below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60065b3b-238f-40ab-8e9b-d39b6d4d8b51",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706216f8-4561-47b8-ae52-97dd4976b33a",
   "metadata": {},
   "source": [
    "To avoid spending time on writing transcripts, we'll use existing data. Let's see if we can get some interesting patterns from it.\n",
    "\n",
    "We'll use [Forever Dreaming](https://foreverdreaming.org/), an online source of online entertainment. The website offers a URL for each BB's episode, from which we can download all the webpage source code. This will allow us to extract the data we want (i.e., transcripts, episode title, number, and season). Once that's done, we can then built a data frame, cleanse it, explore it, analyse it, and make some plots. Let's start by extracting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe3722f-0bf1-4e67-b8a0-7aae6edc9c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries \n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d1a5d7-c0ce-43e5-8d99-4534ed1f9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign URL to a variable\n",
    "base_url = 'https://transcripts.foreverdreaming.org/viewtopic.php?t={}'\n",
    "\n",
    "# create a list of episodes' urls \n",
    "urls = []\n",
    "start_number = 10044\n",
    "end_number = 10107\n",
    "\n",
    "for number in range(start_number, end_number + 1):\n",
    "    episode_url = base_url.format(number)\n",
    "    urls.append(episode_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a545d9ab-3a4d-417a-b9ab-45e4fe67151c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://transcripts.foreverdreaming.org/viewtopic.php?t=10106'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unnecessary urls \n",
    "urls.pop(1)\n",
    "urls.pop(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee087b3-a565-4fc2-a872-c622990194f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d8f59221124a80a3f4f38b07dec4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download episodes' webpages \n",
    "from tqdm.auto import tqdm\n",
    "webpage_texts = [] \n",
    "\n",
    "for url in tqdm(urls):\n",
    "    fid = urllib.request.urlopen(url)\n",
    "    webpage_text = fid.read().decode('utf-8')\n",
    "    webpage_texts.append(webpage_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd80037e-518a-4a40-b3ef-2efd2949984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty lists to store the data \n",
    "transcripts = [] \n",
    "titles = [] \n",
    "seasons = [] \n",
    "episodes = [] \n",
    "\n",
    "# extract transcripts, titles, seasons, and episodes\n",
    "for webpage_text in webpage_texts:\n",
    "    # extract episodes' transcript\n",
    "    transcript_start_idx = webpage_text.find('<div class=\"content\"')\n",
    "    transcript_end_idx = webpage_text.find('<div class=\"share-list\"')\n",
    "    transcript = webpage_text[transcript_start_idx:transcript_end_idx]\n",
    "    transcripts.append(transcript)\n",
    "\n",
    "    # extract episodes' title\n",
    "    title_start_idx = webpage_text.find('<h2 class=\"topic-title\">')\n",
    "    title_end_idx = webpage_text.find('</h2>')\n",
    "    title = webpage_text[title_start_idx:title_end_idx]\n",
    "    pattern = r'<a.*?>(.*?)</a>'\n",
    "    match = re.search(pattern, title)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        episode_title = extracted_text[7:]\n",
    "        titles.append(episode_title)\n",
    "        season_num = int(extracted_text[0])\n",
    "        seasons.append(season_num)\n",
    "        episode_num = int(extracted_text[2:4])\n",
    "        episodes.append(episode_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5fae6-b0d7-4872-9c25-87dff24bbdfc",
   "metadata": {},
   "source": [
    "Now that we've managed to extract the data, we can create the dataframe we will analyse. Below, we're also cleansing the data from unnecessaries words and formats. This will make our analysis easier later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a9c05d-9299-42fe-a71e-edc84a2b8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "df = pd.DataFrame({'Title': titles, 'Season': seasons, 'Episodes': episodes, 'Transcript': transcripts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83df4d1b-5b0d-4100-89e7-09557dcb4c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create empty lists to store the data \n",
    "char_names = []\n",
    "char_sentences = [] \n",
    "sentence_orders = [] \n",
    "episodes_names = [] \n",
    "episodes_numbers = []\n",
    "seasons_numbers = [] \n",
    "\n",
    "# extract characters names, character sentences, etc. \n",
    "for name, episode_number, season_number, transcript in zip(df[\"Title\"], df[\"Episodes\"], df[\"Season\"], df[\"Transcript\"]):\n",
    "    for idx, sentence in enumerate(transcript.split('<br>\\n<br>\\n')):\n",
    "        if '<div class' in sentence:\n",
    "            continue\n",
    "        if '<br>\\n' in sentence:\n",
    "            sentence = sentence.replace('<br>\\n', '')\n",
    "        if '</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t' in sentence:\n",
    "            sentence = sentence.replace('</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t', '')\n",
    "        sentence = sentence.replace(\"Mr: Pinkman:\", \"Mr Pinkman:\")\n",
    "        if any([\":\" in x for x in sentence.split(' ')[:3]]) and ('Then he said' not in sentence):\n",
    "            char_name = sentence.split(':')[0]\n",
    "            char_names.append(char_name)\n",
    "            char_sentence = sentence.split(':')[1]\n",
    "            char_sentences.append(char_sentence)\n",
    "            sentence_orders.append(idx)\n",
    "            episodes_names.append(name)\n",
    "            episodes_numbers.append(episode_number)\n",
    "            seasons_numbers.append(season_number)\n",
    "        else:\n",
    "            char_sentences[-1] = char_sentences[-1] + sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f8c13-9c62-482e-bef9-db780142ef10",
   "metadata": {},
   "source": [
    "We found out that some texts do not have the colon. The colon helps us split the script into character's name and sentence. This is because, in some texts, the character's name was missing. So unfortunately, we can only use Season n1, 2, and 3 up until episode n.8. Season 3 from episode n.9, season 4, and 5 will not be available for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5be199-1d0b-4daf-996e-3d4639f8ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create merged dataframe\n",
    "new_df = pd.DataFrame({'episode_title': episodes_names, 'episode_number': episodes_numbers, 'season_number': seasons_numbers, 'character_name': char_names, 'character_sentence': char_sentences, 'sentence_order':sentence_orders})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b64bd8b4-6689-48ae-ad11-1019c4251b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanse the data\n",
    "new_df.drop(new_df.tail(1).index,inplace=True) \n",
    "new_df = new_df[new_df.character_name != \"Scene\"]\n",
    "new_df[\"character_name\"] = new_df[\"character_name\"].replace(\"Walt\", \"Walter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408afa43-6a8c-42e3-a9e7-0ed18dcd2164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rename the characters column \n",
    "new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == 'Down')] = new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == 'Down')].replace(\"Lawyer\", \"Lawyer_1\")\n",
    "new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == 'Caballo Sin Nombre')] = new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == 'Caballo Sin Nombre')].replace(\"Lawyer\", \"Lawyer_3\")\n",
    "new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == 'One Minute')] = new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == 'One Minute')].replace(\"Lawyer\", \"Lawyer_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb1463b1-42be-4434-a373-cb614b10f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the characters column \n",
    "list_of_episodes = ['No Mas', 'I.F.T.', 'Mas']\n",
    "\n",
    "for episode in list_of_episodes:\n",
    "    new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == episode)] = new_df[(new_df['character_name'] == 'Lawyer') & (new_df['episode_title'] == episode)].replace(\"Lawyer\", \"Lawyer_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3783a1d-81b8-4272-acdb-f73d30d5cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace incorrect names with right names \n",
    "list_of_tuples = [('Walter Junior', 'Walter Jr'), \n",
    "                  ('Gus', 'Gustavo'), \n",
    "                  ('Mr Pinkman', 'Mr. Pinkman'),\n",
    "                  ('Hank(on the news)', 'Hank'), \n",
    "                  ('Janeâ€™s Voicemail', 'Jane'), \n",
    "                  ('Jesse(Answering Machine)', 'Jesse'), \n",
    "                  (' Krazy-8', 'Krazy-8'),\n",
    "                  ('Skyler (Waltâ€™s Imagination)', 'Skyler'), \n",
    "                  ('Walter(Answering Machine)', 'Walter'), \n",
    "                  ('Reporter(on the news)', 'Reporter')\n",
    "                 ]\n",
    "\n",
    "for old_name, new_name in list_of_tuples:\n",
    "    new_df.loc[new_df['character_name'] == old_name, 'character_name'] = new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b64e3e3d-88f5-4ddd-8555-c90717c28958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract length of sentences \n",
    "list_of_len_of_sen = [] \n",
    "\n",
    "for sentence in new_df['character_sentence']:\n",
    "    list_of_len_of_sen.append(len(sentence.split(\" \")))\n",
    "    \n",
    "# create new length of sentences column\n",
    "new_df[\"sentence_length\"] = list_of_len_of_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecb8661b-65f5-442e-a4e7-fd5f0c86b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe into a csv file \n",
    "new_df.to_csv(\"processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbec4e-bf34-4c4f-8fa6-b708e9b3cbfd",
   "metadata": {},
   "source": [
    "We've exported, cleansed, and reformatted the data. Now we're ready to explore it to see if we can find some interesting patterns. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
